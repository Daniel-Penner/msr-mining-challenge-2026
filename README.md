Software development agents such as Claude Code, GitHub Copilot, Cursor Agent, Devin, and OpenAI Codex are being increasingly integrated into developer workflows. While prior work has evaluated agent capabilities for code completion and task automation, little is known about how these agents perform refactoring in practice, the types of changes they make, and their impact on code quality. In this study, we present the first large-scale analysis of agentic refactoring pull requests in Java, comparing them to human refactorings across 86 projects per group. Using RefactoringMiner and DesigniteJava 2.0, we identify refactoring types and detect code smells before and after refactors. Our results demonstrate that agent refactorings are dominated by annotation and documentation-focused changes, in contrast to the diverse structural improvements typical of human developers. These changes often increase code smells, with Claude Code showing the most pronounced increase. Despite differences in refactoring types being performed, our findings indicate that the difference in code smells introduced between agentic and human Java refactors is only statistically significant in Cursor pull requests.
